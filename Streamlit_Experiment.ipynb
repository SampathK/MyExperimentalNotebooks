{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcNMKK/qIF7nL9/vtPVbHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SampathK/MyExperimentalNotebooks/blob/main/Streamlit_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMUH2Xo0bktX"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "pip -Uq install streamlit boto3 streamlit_autorefresh\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import boto3\n",
        "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from streamlit_autorefresh import st_autorefresh\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_ACCESS_KEY_ID = 'your_access_key_id'\n",
        "AWS_SECRET_ACCESS_KEY = 'your_secret_access_key'\n",
        "AWS_BUCKET_NAME = 'your_bucket_name'\n",
        "DYNAMODB_TABLE_NAME = 'your_dynamodb_table_name'\n",
        "REGION_NAME = 'your_aws_region'\n",
        "\n",
        "# Initialize Boto3 clients\n",
        "s3_client = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "dynamodb = boto3.resource(\n",
        "    'dynamodb',\n",
        "    region_name=REGION_NAME,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "table = dynamodb.Table(DYNAMODB_TABLE_NAME)\n",
        "\n",
        "# Function to upload file to S3 and update metadata\n",
        "def upload_file_to_s3(file, bucket_name, file_id, object_name=None):\n",
        "    if object_name is None:\n",
        "        object_name = file.name\n",
        "    try:\n",
        "        # Upload the file to S3\n",
        "        s3_client.upload_fileobj(file, bucket_name, object_name)\n",
        "\n",
        "        # Update the object metadata with file ID\n",
        "        s3_client.copy_object(\n",
        "            Bucket=bucket_name,\n",
        "            CopySource={'Bucket': bucket_name, 'Key': object_name},\n",
        "            Key=object_name,\n",
        "            Metadata={'file_id': file_id},\n",
        "            MetadataDirective='REPLACE'\n",
        "        )\n",
        "        return object_name\n",
        "    except NoCredentialsError:\n",
        "        st.error('Credentials not available')\n",
        "        return None\n",
        "    except PartialCredentialsError:\n",
        "        st.error('Incomplete credentials provided')\n",
        "        return None\n",
        "\n",
        "# Function to insert status into DynamoDB\n",
        "def insert_status(file_id, status, s3_key_summary=None, error_message=None):\n",
        "    timestamp = datetime.utcnow().isoformat()\n",
        "    item = {\n",
        "        'file_id': file_id,\n",
        "        'status_timestamp': timestamp,\n",
        "        'status': status\n",
        "    }\n",
        "    if s3_key_summary:\n",
        "        item['s3_key_summary'] = s3_key_summary\n",
        "    if error_message:\n",
        "        item['error_message'] = error_message\n",
        "\n",
        "    table.put_item(Item=item)\n",
        "\n",
        "# Function to get updated file statuses from DynamoDB\n",
        "def get_updated_statuses(last_check_time):\n",
        "    try:\n",
        "        response = table.scan(\n",
        "            FilterExpression=\"status_timestamp > :last_check_time\",\n",
        "            ExpressionAttributeValues={\":last_check_time\": last_check_time}\n",
        "        )\n",
        "        return response.get('Items', [])\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching updated statuses: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to generate presigned URL\n",
        "def generate_presigned_url(bucket_name, object_key, expiration=3600):\n",
        "    try:\n",
        "        return s3_client.generate_presigned_url('get_object', Params={'Bucket': bucket_name, 'Key': object_key}, ExpiresIn=expiration)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating presigned URL: {e}\")\n",
        "        return None\n",
        "\n",
        "# Streamlit Application\n",
        "st.title(\"Audio File Upload and Processing Pipeline\")\n",
        "\n",
        "# Initialize session state for uploaded files and last check time\n",
        "if 'file_statuses' not in st.session_state:\n",
        "    st.session_state.file_statuses = {}\n",
        "if 'last_check_time' not in st.session_state:\n",
        "    st.session_state.last_check_time = datetime.min.isoformat()\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an audio file\", type=[\"mp3\", \"wav\", \"flac\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    file_id = uploaded_file.name.split('.')[0]\n",
        "    file_details = {\n",
        "        \"Filename\": uploaded_file.name,\n",
        "        \"FileType\": uploaded_file.type,\n",
        "        \"FileSize\": uploaded_file.size\n",
        "    }\n",
        "    st.write(file_details)\n",
        "\n",
        "    # Button to trigger file upload\n",
        "    if st.button(\"Upload and Process\"):\n",
        "        with st.spinner('Uploading...'):\n",
        "            s3_key = upload_file_to_s3(uploaded_file, AWS_BUCKET_NAME, file_id)\n",
        "            if s3_key:\n",
        "                st.success(f'File {uploaded_file.name} uploaded successfully!')\n",
        "\n",
        "                # Insert initial status into DynamoDB\n",
        "                insert_status(file_id, 'File Received')\n",
        "\n",
        "                st.info('File uploaded. Waiting for processing to complete...')\n",
        "\n",
        "# Auto-refresh every 10 seconds to check the status of uploaded files\n",
        "st_autorefresh(interval=10 * 1000, key=\"status_autorefresh\")\n",
        "\n",
        "# Check for updated statuses\n",
        "updated_statuses = get_updated_statuses(st.session_state.last_check_time)\n",
        "if updated_statuses:\n",
        "    st.session_state.last_check_time = max(status['status_timestamp'] for status in updated_statuses)\n",
        "    for status in updated_statuses:\n",
        "        st.session_state.file_statuses[status['file_id']] = status\n",
        "\n",
        "# Display the status of all uploaded files in a table\n",
        "st.header(\"Uploaded Files Status\")\n",
        "\n",
        "if st.session_state.file_statuses:\n",
        "    df = pd.DataFrame(st.session_state.file_statuses.values())\n",
        "    st.table(df[['file_id', 'status_timestamp', 'status']])\n",
        "\n",
        "    # Check if there is a summary file ready for download\n",
        "    for status in st.session_state.file_statuses.values():\n",
        "        if status['status'] == 'Summary Created' and 's3_key_summary' in status:\n",
        "            summary_url = generate_presigned_url(AWS_BUCKET_NAME, status['s3_key_summary'])\n",
        "            st.markdown(f\"[Download Summary for {status['file_id']}]({summary_url})\")\n",
        "else:\n",
        "    st.write(\"No files uploaded yet.\")"
      ],
      "metadata": {
        "id": "dIF8HMgNcFqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import boto3\n",
        "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from streamlit_autorefresh import st_autorefresh\n",
        "from opensearchpy import OpenSearch\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_ACCESS_KEY_ID = 'your_access_key_id'\n",
        "AWS_SECRET_ACCESS_KEY = 'your_secret_access_key'\n",
        "AWS_BUCKET_NAME = 'your_bucket_name'\n",
        "DYNAMODB_TABLE_NAME = 'your_dynamodb_table_name'\n",
        "REGION_NAME = 'your_aws_region'\n",
        "\n",
        "# OpenSearch Configuration\n",
        "OPENSEARCH_HOST = 'your_opensearch_host'\n",
        "OPENSEARCH_PORT = 9200\n",
        "OPENSEARCH_INDEX = 'your_index_name'\n",
        "\n",
        "# Initialize Boto3 clients\n",
        "s3_client = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "dynamodb = boto3.resource(\n",
        "    'dynamodb',\n",
        "    region_name=REGION_NAME,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "table = dynamodb.Table(DYNAMODB_TABLE_NAME)\n",
        "\n",
        "# Initialize OpenSearch client\n",
        "client = OpenSearch(\n",
        "    hosts=[{'host': OPENSEARCH_HOST, 'port': OPENSEARCH_PORT}],\n",
        "    http_auth=('your_username', 'your_password'),\n",
        "    use_ssl=True,\n",
        "    verify_certs=True,\n",
        "    ssl_show_warn=False\n",
        ")\n",
        "\n",
        "# Function to upload file to S3 and update metadata\n",
        "def upload_file_to_s3(file, bucket_name, file_id, object_name=None):\n",
        "    if object_name is None:\n",
        "        object_name = file.name\n",
        "    try:\n",
        "        # Upload the file to S3\n",
        "        s3_client.upload_fileobj(file, bucket_name, object_name)\n",
        "\n",
        "        # Update the object metadata with file ID\n",
        "        s3_client.copy_object(\n",
        "            Bucket=bucket_name,\n",
        "            CopySource={'Bucket': bucket_name, 'Key': object_name},\n",
        "            Key=object_name,\n",
        "            Metadata={'file_id': file_id},\n",
        "            MetadataDirective='REPLACE'\n",
        "        )\n",
        "        return object_name\n",
        "    except NoCredentialsError:\n",
        "        st.error('Credentials not available')\n",
        "        return None\n",
        "    except PartialCredentialsError:\n",
        "        st.error('Incomplete credentials provided')\n",
        "        return None\n",
        "\n",
        "# Function to insert status into DynamoDB\n",
        "def insert_status(file_id, status, s3_key_summary=None, error_message=None):\n",
        "    timestamp = datetime.utcnow().isoformat()\n",
        "    item = {\n",
        "        'file_id': file_id,\n",
        "        'status_timestamp': timestamp,\n",
        "        'status': status\n",
        "    }\n",
        "    if s3_key_summary:\n",
        "        item['s3_key_summary'] = s3_key_summary\n",
        "    if error_message:\n",
        "        item['error_message'] = error_message\n",
        "\n",
        "    table.put_item(Item=item)\n",
        "\n",
        "# Function to get updated file statuses from DynamoDB\n",
        "def get_updated_statuses(last_check_time):\n",
        "    try:\n",
        "        response = table.scan(\n",
        "            FilterExpression=\"status_timestamp > :last_check_time\",\n",
        "            ExpressionAttributeValues={\":last_check_time\": last_check_time}\n",
        "        )\n",
        "        return response.get('Items', [])\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching updated statuses: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to generate presigned URL\n",
        "def generate_presigned_url(bucket_name, object_key, expiration=3600):\n",
        "    try:\n",
        "        return s3_client.generate_presigned_url('get_object', Params={'Bucket': bucket_name, 'Key': object_key}, ExpiresIn=expiration)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating presigned URL: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to fetch tags from OpenSearch\n",
        "def fetch_tags_from_opensearch():\n",
        "    query = {\n",
        "        \"size\": 1000,  # Adjust size as needed\n",
        "        \"_source\": [\"tags\"],\n",
        "        \"query\": {\n",
        "            \"match_all\": {}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = client.search(index=OPENSEARCH_INDEX, body=query)\n",
        "    tags = []\n",
        "    for hit in response['hits']['hits']:\n",
        "        tags.extend(hit['_source']['tags'])\n",
        "\n",
        "    return tags\n",
        "\n",
        "# Function to generate word cloud from tags\n",
        "def generate_word_cloud(tags):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(tags))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Streamlit Application\n",
        "st.title(\"Audio File Upload and Processing Pipeline\")\n",
        "\n",
        "# Initialize session state for uploaded files and last check time\n",
        "if 'file_statuses' not in st.session_state:\n",
        "    st.session_state.file_statuses = {}\n",
        "if 'last_check_time' not in st.session_state:\n",
        "    st.session_state.last_check_time = datetime.min.isoformat()\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an audio file\", type=[\"mp3\", \"wav\", \"flac\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    file_id = uploaded_file.name.split('.')[0]\n",
        "    file_details = {\n",
        "        \"Filename\": uploaded_file.name,\n",
        "        \"FileType\": uploaded_file.type,\n",
        "        \"FileSize\": uploaded_file.size\n",
        "    }\n",
        "    st.write(file_details)\n",
        "\n",
        "    # Button to trigger file upload\n",
        "    if st.button(\"Upload and Process\"):\n",
        "        with st.spinner('Uploading...'):\n",
        "            s3_key = upload_file_to_s3(uploaded_file, AWS_BUCKET_NAME, file_id)\n",
        "            if s3_key:\n",
        "                st.success(f'File {uploaded_file.name} uploaded successfully!')\n",
        "\n",
        "                # Insert initial status into DynamoDB\n",
        "                insert_status(file_id, 'File Received')\n",
        "\n",
        "                st.info('File uploaded. Waiting for processing to complete...')\n",
        "\n",
        "# Auto-refresh every 10 seconds to check the status of uploaded files\n",
        "st_autorefresh(interval=10 * 1000, key=\"status_autorefresh\")\n",
        "\n",
        "# Check for updated statuses\n",
        "updated_statuses = get_updated_statuses(st.session_state.last_check_time)\n",
        "if updated_statuses:\n",
        "    st.session_state.last_check_time = max(status['status_timestamp'] for status in updated_statuses)\n",
        "    for status in updated_statuses:\n",
        "        st.session_state.file_statuses[status['file_id']] = status\n",
        "\n",
        "# Display the status of all uploaded files in a table\n",
        "st.header(\"Uploaded Files Status\")\n",
        "\n",
        "if st.session_state.file_statuses:\n",
        "    df = pd.DataFrame(st.session_state.file_statuses.values())\n",
        "    st.table(df[['file_id', 'status_timestamp', 'status']])\n",
        "\n",
        "    # Check if there is a summary file ready for download\n",
        "    for status in st.session_state.file_statuses.values():\n",
        "        if status['status'] == 'Summary Created' and 's3_key_summary' in status:\n",
        "            summary_url = generate_presigned_url(AWS_BUCKET_NAME, status['s3_key_summary'])\n",
        "            st.markdown(f\"[Download Summary for {status['file_id']}]({summary_url})\")\n",
        "else:\n",
        "    st.write(\"No files uploaded yet.\")\n",
        "\n",
        "# Section to generate word cloud from OpenSearch tags\n",
        "st.header(\"Generate Word Cloud from OpenSearch Tags\")\n",
        "\n",
        "if st.button(\"Generate Word Cloud\"):\n",
        "    with st.spinner('Fetching tags from OpenSearch...'):\n",
        "        tags = fetch_tags_from_opensearch()\n",
        "        if tags:\n",
        "            st.success('Tags fetched successfully!')\n",
        "            st.write(\"Tags:\", tags)\n",
        "\n",
        "            with st.spinner('Generating word cloud...'):\n",
        "                generate_word_cloud(tags)\n",
        "                st.pyplot(plt)\n",
        "        else:\n",
        "            st.warning('No tags found in OpenSearch.')\n"
      ],
      "metadata": {
        "id": "SEuNVyyxp7-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opensearch-py wordcloud streamlit"
      ],
      "metadata": {
        "id": "rgZ7mw8KqT4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Here's the updated Streamlit application code that accepts `file_id` and `email` as input before uploading the file to S3 and updates the same information in the S3 metadata:\n",
        "\n",
        "```python\n",
        "import streamlit as st\n",
        "import boto3\n",
        "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from streamlit_autorefresh import st_autorefresh\n",
        "from opensearchpy import OpenSearch\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_ACCESS_KEY_ID = 'your_access_key_id'\n",
        "AWS_SECRET_ACCESS_KEY = 'your_secret_access_key'\n",
        "AWS_BUCKET_NAME = 'your_bucket_name'\n",
        "DYNAMODB_TABLE_NAME = 'your_dynamodb_table_name'\n",
        "REGION_NAME = 'your_aws_region'\n",
        "\n",
        "# OpenSearch Configuration\n",
        "OPENSEARCH_HOST = 'your_opensearch_host'\n",
        "OPENSEARCH_PORT = 9200\n",
        "OPENSEARCH_INDEX = 'your_index_name'\n",
        "\n",
        "# Initialize Boto3 clients\n",
        "s3_client = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "dynamodb = boto3.resource(\n",
        "    'dynamodb',\n",
        "    region_name=REGION_NAME,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "table = dynamodb.Table(DYNAMODB_TABLE_NAME)\n",
        "\n",
        "# Initialize OpenSearch client\n",
        "client = OpenSearch(\n",
        "    hosts=[{'host': OPENSEARCH_HOST, 'port': OPENSEARCH_PORT}],\n",
        "    http_auth=('your_username', 'your_password'),\n",
        "    use_ssl=True,\n",
        "    verify_certs=True,\n",
        "    ssl_show_warn=False\n",
        ")\n",
        "\n",
        "# Function to upload file to S3 and update metadata\n",
        "def upload_file_to_s3(file, bucket_name, file_id, email, object_name=None):\n",
        "    if object_name is None:\n",
        "        object_name = file.name\n",
        "    try:\n",
        "        # Upload the file to S3\n",
        "        s3_client.upload_fileobj(file, bucket_name, object_name)\n",
        "\n",
        "        # Update the object metadata with file ID and email\n",
        "        s3_client.copy_object(\n",
        "            Bucket=bucket_name,\n",
        "            CopySource={'Bucket': bucket_name, 'Key': object_name},\n",
        "            Key=object_name,\n",
        "            Metadata={'file_id': file_id, 'email': email},\n",
        "            MetadataDirective='REPLACE'\n",
        "        )\n",
        "        return object_name\n",
        "    except NoCredentialsError:\n",
        "        st.error('Credentials not available')\n",
        "        return None\n",
        "    except PartialCredentialsError:\n",
        "        st.error('Incomplete credentials provided')\n",
        "        return None\n",
        "\n",
        "# Function to insert status into DynamoDB\n",
        "def insert_status(file_id, status, email, s3_key_summary=None, error_message=None):\n",
        "    timestamp = datetime.utcnow().isoformat()\n",
        "    item = {\n",
        "        'file_id': file_id,\n",
        "        'email': email,\n",
        "        'status_timestamp': timestamp,\n",
        "        'status': status\n",
        "    }\n",
        "    if s3_key_summary:\n",
        "        item['s3_key_summary'] = s3_key_summary\n",
        "    if error_message:\n",
        "        item['error_message'] = error_message\n",
        "\n",
        "    table.put_item(Item=item)\n",
        "\n",
        "# Function to get updated file statuses from DynamoDB\n",
        "def get_updated_statuses(last_check_time):\n",
        "    try:\n",
        "        response = table.scan(\n",
        "            FilterExpression=\"status_timestamp > :last_check_time\",\n",
        "            ExpressionAttributeValues={\":last_check_time\": last_check_time}\n",
        "        )\n",
        "        return response.get('Items', [])\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching updated statuses: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to generate presigned URL\n",
        "def generate_presigned_url(bucket_name, object_key, expiration=3600):\n",
        "    try:\n",
        "        return s3_client.generate_presigned_url('get_object', Params={'Bucket': bucket_name, 'Key': object_key}, ExpiresIn=expiration)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating presigned URL: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to fetch tags from OpenSearch\n",
        "def fetch_tags_from_opensearch():\n",
        "    query = {\n",
        "        \"size\": 1000,  # Adjust size as needed\n",
        "        \"_source\": [\"tags\"],\n",
        "        \"query\": {\n",
        "            \"match_all\": {}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = client.search(index=OPENSEARCH_INDEX, body=query)\n",
        "    tags = []\n",
        "    for hit in response['hits']['hits']:\n",
        "        tags.extend(hit['_source']['tags'])\n",
        "\n",
        "    return tags\n",
        "\n",
        "# Function to generate word cloud from tags\n",
        "def generate_word_cloud(tags):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(tags))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Streamlit Application\n",
        "st.title(\"Audio File Upload and Processing Pipeline\")\n",
        "\n",
        "# Initialize session state for uploaded files and last check time\n",
        "if 'file_statuses' not in st.session_state:\n",
        "    st.session_state.file_statuses = {}\n",
        "if 'last_check_time' not in st.session_state:\n",
        "    st.session_state.last_check_time = datetime.min.isoformat()\n",
        "\n",
        "# Input fields for file_id and email\n",
        "file_id = st.text_input(\"Enter File ID\")\n",
        "email = st.text_input(\"Enter Email\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an audio file\", type=[\"mp3\", \"wav\", \"flac\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    file_details = {\n",
        "        \"Filename\": uploaded_file.name,\n",
        "        \"FileType\": uploaded_file.type,\n",
        "        \"FileSize\": uploaded_file.size\n",
        "    }\n",
        "    st.write(file_details)\n",
        "\n",
        "    # Button to trigger file upload\n",
        "    if st.button(\"Upload and Process\"):\n",
        "        with st.spinner('Uploading...'):\n",
        "            s3_key = upload_file_to_s3(uploaded_file, AWS_BUCKET_NAME, file_id, email)\n",
        "            if s3_key:\n",
        "                st.success(f'File {uploaded_file.name} uploaded successfully!')\n",
        "\n",
        "                # Insert initial status into DynamoDB\n",
        "                insert_status(file_id, 'File Received', email)\n",
        "\n",
        "                st.info('File uploaded. Waiting for processing to complete...')\n",
        "\n",
        "# Auto-refresh every 10 seconds to check the status of uploaded files\n",
        "st_autorefresh(interval=10 * 1000, key=\"status_autorefresh\")\n",
        "\n",
        "# Check for updated statuses\n",
        "updated_statuses = get_updated_statuses(st.session_state.last_check_time)\n",
        "if updated_statuses:\n",
        "    st.session_state.last_check_time = max(status['status_timestamp'] for status in updated_statuses)\n",
        "    for status in updated_statuses:\n",
        "        st.session_state.file_statuses[status['file_id']] = status\n",
        "\n",
        "# Display the status of all uploaded files in a table\n",
        "st.header(\"Uploaded Files Status\")\n",
        "\n",
        "if st.session_state.file_statuses:\n",
        "    df = pd.DataFrame(st.session_state.file_statuses.values())\n",
        "    st.table(df[['file_id', 'email', 'status_timestamp', 'status']])\n",
        "\n",
        "    # Check if there is a summary file ready for download\n",
        "    for status in st.session_state.file_statuses.values():\n",
        "        if status['status'] == 'Summary Created' and 's3_key_summary' in status:\n",
        "            summary_url = generate_presigned_url(AWS_BUCKET_NAME, status['s3_key_summary'])\n",
        "            st.markdown(f\"[Download Summary for {status['file_id']}]({summary_url})\")\n",
        "else:\n",
        "    st.write(\"No files uploaded yet.\")\n",
        "\n",
        "# Section to generate word cloud from OpenSearch tags\n",
        "st.header(\"Generate Word Cloud from OpenSearch Tags\")\n",
        "\n",
        "if st.button(\"Generate Word Cloud\"):\n",
        "    with st.spinner('Fetching tags from OpenSearch...'):\n",
        "        tags = fetch_tags_from_opensearch()\n",
        "        if tags:\n",
        "            st.success('Tags fetched successfully!')\n",
        "            st.write(\"Tags:\", tags)\n",
        "\n",
        "            with st.spinner('Generating word cloud...'):\n",
        "                generate_word_cloud(tags)\n",
        "                st.pyplot(plt)\n",
        "        else:\n",
        "            st.warning('No tags found in OpenSearch.')\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "1. **Input Fields for File ID and Email:**\n",
        "   - Added text input fields for `file_id` and `email` before the file uploader.\n",
        "\n",
        "2. **Upload and Process Button:**\n",
        "   - Updated the `upload_file_to_s3` function to accept `file_id` and `email` and store them in the S3 metadata.\n",
        "   - Updated the `insert_status` function to include `email` in the DynamoDB entry.\n",
        "\n",
        "3. **Display the Status:**\n",
        "   - Updated the status table to include `email`.\n",
        "\n",
        "This setup ensures that the file ID and email are input before uploading the file to S3 and are stored in the S3 metadata and DynamoDB."
      ],
      "metadata": {
        "id": "oN_H2L8guUO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}